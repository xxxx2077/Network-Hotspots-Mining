'''
è¯·å…ˆæ›´æ–°å¿…è¦çš„åº“
pip install transformers
pip install torch
pip install jieba
'''
import os
import time

from transformers import BertTokenizer, BertModel
import torch
import jieba
import re
import numpy as np
from SchoolMarket.utils.database import MysqlOperator

'''
å¯é€‰é¢„è®­ç»ƒæ¨¡å‹æœ‰ï¼š
RoBERTa-wwm-extï¼šå…¨ç§°RoBERTa-wwm-extï¼ˆWhole Word Maskingï¼‰ï¼Œåœ¨å¤§è§„æ¨¡ä¸­æ–‡è¯­æ–™ä¸Šè¿›è¡Œäº†æ›´é•¿æ—¶é—´çš„è®­ç»ƒï¼Œæ•ˆæœä¼˜äºBERT-base-chineseã€‚
MacBERTï¼šMacBERTæ˜¯åœ¨BERTå’ŒRoBERTaçš„åŸºç¡€ä¸Šè¿›è¡Œæ”¹è¿›çš„ä¸€ä¸ªä¸­æ–‡æ¨¡å‹ï¼Œåœ¨è®¸å¤šNLPä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚
ERNIEï¼šç™¾åº¦å¼€å‘çš„ERNIEæ¨¡å‹ï¼Œé€šè¿‡å¼•å…¥çŸ¥è¯†å›¾è°±è¿›ä¸€æ­¥å¢å¼ºäº†è¯­ä¹‰ç†è§£èƒ½åŠ›ã€‚
'''

# 1ï¼š

# # åŠ è½½é¢„è®­ç»ƒçš„ERNIEæ¨¡å‹å’Œåˆ†è¯å™¨
# tokenizer = BertTokenizer.from_pretrained('nghuyong/ernie-1.0')
# model = BertModel.from_pretrained('nghuyong/ernie-1.0')

# åŠ è½½é¢„è®­ç»ƒçš„MacBERTæ¨¡å‹å’Œåˆ†è¯å™¨
# tokenizer = BertTokenizer.from_pretrained('hfl/chinese-macbert-base')
# model = BertModel.from_pretrained('hfl/chinese-macbert-base')

# ç”¨ç»å¯¹è·¯å¾„ä¸æ˜“å‡ºé”™
current_dir = os.path.dirname(os.path.abspath(__file__))
model_dir = os.path.join(current_dir, 'tmp/MacBERT')

# ä¿å­˜æ¨¡å‹å’Œåˆ†è¯å™¨
# model.save_pretrained(model_dir)
# tokenizer.save_pretrained(model_dir)


# 2ï¼š
# åŠ è½½ä¿å­˜çš„æ¨¡å‹å’Œåˆ†è¯å™¨

tokenizer = BertTokenizer.from_pretrained(model_dir)
model = BertModel.from_pretrained(model_dir)

# 3ï¼š
# å®šä¹‰é«˜æ ¡ç›¸å…³çš„è¯æ¡é›†åˆ
university_related_terms = [
    "æ¦•å›­", "è”å›­", "ç‘¾å›­",
    "é£Ÿå ‚", "æ ¡é—¨", "å®¿èˆ", "å·¥åœ°", "å­¦é™¢", "è¯¾ç¨‹", "è€ƒè¯•", "ä¸“ä¸š",
    "æ•™æˆ", "è€å¸ˆ", "è¡Œæ”¿", "æ ¡é•¿", "å­¦æ ¡", "ä¸­å±±å¤§å­¦", "é¸­å¤§", "ä¸­å¤§",
    "ä¸­ç ", "ç æµ·æ ¡åŒº", "å—æ ¡", "ä¸œæ ¡", "åŒ—æ ¡", "ä¸­æ·±", "æ·±åœ³æ ¡åŒº",
    "åŒ»å­¦é™¢", "æ–‡å­¦é™¢", "ç†å­¦é™¢", "å·¥å­¦é™¢", "æ•°æ®ç§‘å­¦ä¸è®¡ç®—æœºå­¦é™¢", "æ–°åå­¦é™¢", "ä¼ æ’­ä¸è®¾è®¡å­¦é™¢", "å²­å—å­¦é™¢",
    "å›½é™…ç»æµè´¸æ˜“å­¦é™¢", "æ³•å­¦é™¢", "å•†å­¦é™¢", "é©¬å…‹æ€ä¸»ä¹‰å­¦é™¢", "å›½é™…å­¦é™¢", "æ—…æ¸¸å­¦é™¢", "ææ–™å­¦é™¢", "è¯å­¦é™¢",
    "ç”Ÿå‘½ç§‘å­¦å­¦é™¢", "ç¯å¢ƒä¸èƒ½æºå­¦é™¢", "ç”Ÿç‰©åŒ»å­¦å·¥ç¨‹å­¦é™¢", "å¾®ç”µå­å­¦é™¢", "è½¯ä»¶å­¦é™¢", "å…¬å…±å«ç”Ÿå­¦é™¢", "æµ·æ´‹å­¦é™¢",
    "ç¤¾ä¼šç§‘å­¦é™¢",
    "å¤§å­¦ç”Ÿæ´»åŠ¨ä¸­å¿ƒ", "ä½“è‚²é¦†", "å›¾ä¹¦é¦†", "ç§‘å­¦ä¼šå ‚", "ç ”ç©¶ç”Ÿé™¢"
]


# 4ï¼š
# è·å–è¯æ¡é›†åˆçš„BERTåµŒå…¥
def get_embeddings(text_list, tokenizer, model):
    inputs = tokenizer(text_list, max_length=512, return_tensors='pt', padding=True, truncation=True)
    outputs = model(**inputs)
    embeddings = outputs.last_hidden_state.mean(dim=1)
    return embeddings


# è·å–ç›¸å…³è¯æ¡çš„åµŒå…¥
related_terms_embeddings = get_embeddings(university_related_terms, tokenizer, model)

# è®¾ç½®æ—¥å¿—ç­‰çº§
jieba.setLogLevel(jieba.logging.INFO)
# å»é™¤åœç”¨è¯çš„ä¸­æ–‡åˆ†è¯
def chinese_word_cut(mytext, stopwords="all"):
    # jieba.load_userdict('è‡ªå®šä¹‰è¯å…¸.txt')  # è¿™é‡Œä½ å¯ä»¥æ·»åŠ jiebaåº“è¯†åˆ«ä¸äº†çš„ç½‘ç»œæ–°è¯ï¼Œé¿å…å°†ä¸€äº›æ–°è¯æ‹†å¼€

    # æ–‡æœ¬é¢„å¤„ç† ï¼šå»é™¤ä¸€äº›æ— ç”¨çš„å­—ç¬¦åªæå–å‡ºä¸­æ–‡å‡ºæ¥
    new_data = re.findall('[\u4e00-\u9fa5]+', mytext, re.S)
    new_data = " ".join(new_data)
    # æ–‡æœ¬åˆ†è¯
    seg_list_exact = jieba.lcut(new_data)
    result_list = []
    # è¯»å–åœç”¨è¯åº“
    stopwords_path = os.path.join(current_dir, "stopwords", f"stopwords_{stopwords}.txt")
    # print("path",stopwords_path)
    with open(stopwords_path, encoding='utf-8') as f:  # å¯æ ¹æ®éœ€è¦æ‰“å¼€åœç”¨è¯åº“ï¼Œç„¶ååŠ ä¸Šä¸æƒ³æ˜¾ç¤ºçš„è¯è¯­
        con = f.readlines()
        stop_words = set()
        for i in con:
            i = i.replace("\n", "")  # å»æ‰è¯»å–æ¯ä¸€è¡Œæ•°æ®çš„\n
            stop_words.add(i)
    # å»é™¤åœç”¨è¯
    for word in seg_list_exact:
        if word not in stop_words:
            result_list.append(word)
    return result_list


# 5ï¼š
# è®¡ç®—å¸–å­å†…å®¹ä¸é«˜æ ¡çš„ç›¸å…³ç¨‹åº¦
def calculate_relevance(post_content, related_terms_embeddings=related_terms_embeddings, tokenizer=tokenizer,
                        model=model):
    # å¯¹å¸–å­å†…å®¹è¿›è¡Œåˆ†è¯
    # tokens = jieba.lcut(post_content)
    # å»é™¤åœç”¨è¯å†åˆ†è¯
    tokens = chinese_word_cut(post_content)
    post_content = ' '.join(tokens)
    # print("token", tokens)
    # è·å–å¸–å­å†…å®¹çš„BERTåµŒå…¥
    post_embedding = get_embeddings([post_content], tokenizer, model)
    # print(post_embedding)
    # è®¡ç®—å¸–å­å†…å®¹åµŒå…¥ä¸ç›¸å…³è¯æ¡åµŒå…¥çš„ä½™å¼¦ç›¸ä¼¼åº¦
    similarities = torch.nn.functional.cosine_similarity(post_embedding, related_terms_embeddings)

    # è®¡ç®—ç›¸ä¼¼åº¦çš„æœ€å¤§å€¼ä½œä¸ºç›¸å…³ç¨‹åº¦
    # relevance = similarities.max().item()
    # print(relevance)
    relevance = similarities.mean().item()
    # print(relevance)
    return relevance


def update_relevance(db_opr: MysqlOperator):
    select_sql = "select id, title, content from post order by time desc limit 3500,3500;"
    rows = db_opr.query(select_sql)
    count = 0
    for row in rows:
        id, title, content = row
        relevance = calculate_relevance(title + content)
        update_sql = f" update post set correlation = {relevance} where id = {id};"
        db_opr.exec_sql(update_sql)
        count += 1
        if count % 100 == 0:
            print(count)

def update_null_relevance(db_opr: MysqlOperator):
    select_sql = "select id, title, content from post where correlation is null;"
    rows = db_opr.query(select_sql)
    for row in rows:
        id, title, content = row
        relevance = calculate_relevance(title + content)
        update_sql = f" update post set correlation = {relevance} where id = {id};"
        db_opr.exec_sql(update_sql)



def main():
    # ç¤ºä¾‹å¸–å­å†…å®¹
    post_content = "ä»Šå¤©åœ¨å­¦æ ¡çš„é£Ÿå ‚åƒåˆ°äº†ç‰¹åˆ«å¥½åƒçš„é¥­èœã€‚"
    #     post_content = """
    # æ”¶çº¸ç®±ğŸ“¦ï¼Œ60x40x50
    #     """
    # post_content = ""
    print(post_content)
    # è®¡ç®—å¸–å­å†…å®¹ä¸é«˜æ ¡çš„ç›¸å…³ç¨‹åº¦
    relevance_score = calculate_relevance(post_content, related_terms_embeddings, tokenizer, model)
    print(f"Relevance Score: {relevance_score}")

    # print(time.strftime("%Y-%m-%d %H:%M:%S"))
    # opr = MysqlOperator()
    # update_relevance(opr)
    # opr.close()
    # print(time.strftime("%Y-%m-%d %H:%M:%S"))


if __name__ == "__main__":
    main()
