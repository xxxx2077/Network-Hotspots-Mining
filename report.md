# Development report



## 运行须知

子项目的readme文件中有的东西可以不写，说明一下该子项目负责的模块功能就行

### 爬虫



### 数据库



### 后端

安装依赖

```
pip install -r requirements.txt
```

启动项目

```
python3 manage.py runserver
```



### 前端

在`hotpoints`目录下：

- 安装依赖

  ```bash
  npm install
  ```

- 启动

  ```bash
  npm run serve
  ```

### 大模型

#### 1. 大模型参数和运行配置

* **Baichuan2-13B-4bits**：
  * 参数量为13B的模型
  * 并进行了4bits的量化，提升了模型的运行效率和内存使用率，使得模型可以在 **20GB 显存**的限制下运行。
  * 加载基于IEPile训练的 [LoRA 权重](https://huggingface.co/zjunlp/baichuan2-13b-iepile-lora)，针对性地对**事件提取**等任务进行专门优化，并且有效地**稳定输出格式**，便于后端处理。
* **推荐配置**：
  * NVIDIA RTX3090/4090 及以上
  * 显存大于 20 GB

#### 2. 运行

在`Baichuan2`目录下：

* 安装依赖

  ```bash
  pip install -r requirements.txt
  ```

* 启动 ngrok 内网穿透

  ```bash
  ngrok http --domain=akita-famous-sincerely.ngrok-free.app 8000
  ```

* 启动 Baichuan2 大模型 API

  ```bash
  python OpenAI_api.py
  ```




## 代码结构

主要保留二次开发的可能，说明开发需要用到的某某文件用处用途，写完后我会统一放在子文件的readme里

### 爬虫



### 数据库



### 后端

```
├── Network_Hotspots_Mining         # 项目根目录，项目名为Network Hotspots Mining
│   ├── app                         # Django应用目录
│   │   ├── admin.py                	# Django admin后台配置
│   │   ├── apps.py                 	# Django应用定义文件
│   │   ├── controller              	# 控制器模块，存放处理业务逻辑的脚本
│   │   │   ├── LLM.py              		# 处理语言模型相关功能的脚本
│   │   │   ├── __pycache__          		# Python编译后的字节码缓存目录
│   │   │   │   ├── LLM.cpython-310.pyc  		# LLM.py对应的字节码文件
│   │   │   │   └── single_pass.cpython-310.pyc 	# single_pass.py对应的字节码文件
│   │   │   └── single_pass.py      		# single-pass聚类算法文件
│   │   ├── data                    	# 存放数据文件的目录
│   │   │   ├── data1.txt           		# 示例数据文件1
│   │   │   └── stop_words.txt      		# 停用词文件，常用于文本预处理
│   │   ├── __init__.py             	# 初始化文件，用于定义模块属性
│   │   ├── migrations              	# 数据库迁移文件目录，记录模型变更历史
│   │   │   ├── 0001_initial.py     		# 初始数据库迁移脚本
│   │   │   ├── __init__.py         		# 迁移模块初始化文件
│   │   │   └── __pycache__         		# 迁移脚本的字节码缓存
│   │   ├── misc                    	# 杂项或辅助脚本目录
│   │   │   ├── clear_db.py         		# 清空数据库脚本
│   │   │   ├── data_processing.py  		# 数据处理脚本
│   │   │   ├── __pycache__         		# 杂项脚本的字节码缓存
│   │   │   └── test.py             		# 测试脚本或示例脚本
│   │   ├── models.py               	# 定义Django模型（数据库表结构）的文件
│   │   ├── __pycache__             	# app模块中各py文件的字节码缓存
│   │   ├── result                  	# 存放处理结果或报告的目录
│   │   │   ├── ...                 		# 各种结果文件
│   │   ├── tasks.py                	# Celery异步任务定义文件
│   │   ├── tests.py                	# 单元测试文件
│   │   ├── util                    	# 工具函数或模块目录
│   │   │   ├── data_analysis.py    		# 数据分析工具脚本
│   │   │   ├── __pycache__         		# 工具模块的字节码缓存
│   │   │   └── util.py             		# 工具函数主文件
│   │   └── views.py                	# 视图函数定义，处理HTTP请求和响应
│   ├── manage.py                   # Django项目管理命令入口
│   └── Network_Hotspots_Mining     # 与项目同名的目录，可能包含了项目级别的配置
│       ├── asgi.py                 	# ASGI服务器配置文件，用于部署Web应用
│       ├── celery.py               	# Celery配置文件，用于任务队列设置
│       ├── __init__.py             	# 项目级别初始化文件
│       ├── __pycache__             	# 项目级别配置文件的字节码缓存
│       ├── settings.py             	# Django项目设置文件
│       ├── urls.py                 	# URL路由配置文件
│       └── wsgi.py                 	# WSGI服务器入口文件，用于生产环境部署
├── README.md                      # 项目说明文档，包含安装、使用等指导信息
└── requirements.txt               # 项目依赖列表文件，用于pip安装所需第三方库
```



### 前端

```
src/
│
├── api/					// 网络请求
│   ├── xxx.js
│   └── xxx.js
│
├── assets/					// 资源文件
│   └── xxx
│
├── components/				// 组件
│   ├── xxx.vue/
│   ...
│   └── xxx.vue2/
│
├── router/					// 路由
│   └── index.js
│
├── utils/					// 自定义功能函数
│   └── xxx
│
├── views/					// 页面文件
│   ├── HomeView/
│   └── Topic/
│
└── App.vue					// 入口文件
```

### 大模型

```
Baichuan2/
│
├── baichuan-inc/					// LoRA 权重
│   ├── Baichuan2-7B-Chat-4bits
│
└── OpenAI_api.py					// 大模型 API 文件
```



## 技术报告

展示技术工作以及个人贡献，顺序按ppt的来，简单讲讲用的什么怎么用的就行，留点开发建议。涉及个人工作的东西不会提交到最终的github上。

### 整体架构（不用写

### zjt

### sdp

### ywx

#### 调研阶段

调研事件总结、事件抽取方法，产出调研知识文档，提出尝试deepKE进行事件抽取的建议。

针对网络热点文本聚类方法和对Python语言下的后端框架进行调研，对实现难度和性能进行评估。

#### 开发阶段

个人从零搭建项目后端部分，完成后端部分开发。

- 实现后端与数据库对接，使后端能从数据库读取数据
- 基于SIngle-Pass算法实现新闻热点聚类
  - 设置停用词列表，形式为文件，通过读取文件可获取停用词列表。 
  - 切割句子cut_sentences： 接受文本或文本文件路径作为输入，使用jieba进行分词处理，并将分词后的文本转换为“空格+词”的字符串形式，同时构建文本ID到文本内容的映射。
  - 获取TF-IDF方法 get_tfidf： 将分词后的文本转换为TF-IDF矩阵表示，并将其转换为稠密列表形式返回。
  - 余弦相似度计算方法 cosion_simi： 计算给定向量与所有簇中心向量之间的余弦相似度，并返回最大相似度值及其对应的簇索引。 
  - 单遍聚类方法 single_pass： 对输入文本执行single_pass聚类算法。首先调用cut_sentences和get_tfidf方法预处理文本，然后遍历每个文本的TF-IDF向量，根据与已有簇中心的相似度决定是否加入现有簇或创建新簇。最后，将聚类结果保存到指定的JSON文件中。
- 与大模型API对接
  - 使用线程池并行技术对调用大模型API函数进行性能优化
  - 调用LLM API对事件聚类进行总结的同时计算每个聚类的热度和热度上升率
- 接口开发
  - 获取热度上升榜get_speedlist
  - 获取热度总值榜get_hotlist。
- 编写测试脚本（misc目录）
  - 数据预处理脚本data_processing.py
  - 清空数据库指定库表脚本clear_db.py
  - 测试部分后端接口功能脚本test.py

### hwy

#### 背景调研：任务

相比传统机器学习模型，大模型通过大量的数据预训练，具备了强大的**泛化能力**，可以完成多种传统机器学习模型难以完成的任务，如事件总结、事件关系判断等，并表现出更高的准确性和稳定性。

除此之外，大模型还具有**自动特征提取**的能力。通过大量的数据的训练，大模型能够自动提取和学习数据中的特征，而无需人工设计复杂的特征工程。这极大地简化了开发流程，提升了开发效率。这也非常适合本项目的任务特征。

因此，我们使用大模型完成项目中的3个任务：

* **事件总结**：利用大模型的深度学习能力，自动识别并总结事件的关键内容和重要信息。
* **聚类总结**：利用大模型处理聚类算法的结果，对每一类进行总结。
* **事件关系**：利用大模型理解和分析事件之间的关系，如前提、结果、补充等。这为**知识图谱**的构建提供了必要的信息。

#### 背景调研：开源大模型对比

在选择适合项目的大模型时，我们需要考虑多个因素，包括模型的语言支持、参数量、灵活性以及适应特定任务的能力。由于项目的计算资源有限，为了平衡效率和精度，我们统一选择参数量为7B左右，或者量化后的13B模型进行测试。

* **OneKE**：
  * **开发者/机构**：由蚂蚁集团和浙江大学联合研发。
  * **主要特点**：这是一个专注于知识抽取的大模型框架，适用于处理结构化知识提取任务。
  * **放弃原因**：尽管OneKE在知识抽取方面表现出色，但它在处理复杂的事件关系任务时灵活度不足，不支持自定义事件关系的抽取和分析，无法建立知识图谱。
* **LLaMA2-7B**：
  * **主要特点**：LLaMA2-7B是一款性能优异的通用大模型，参数量为7B，适用于多种语言处理任务。
  * **放弃原因**：虽然LLaMA2-7B在多语言处理上具有较强的能力，但其在中文任务上的表现并不足以满足我们的需求。由于本次项目主要面向中文环境，需要一个对中文有更深入优化的模型。
* **Baichuan2-13B-4bits**：
  * **主要特点**：Baichuan2是一款专门为中文优化的大模型，参数量为13B但经过4bits量化处理，使得模型既保持了高精度也适应了有限的计算资源。
  * **选择理由**：Baichuan2-13B-4bits的优异中文处理能力以及其量化后的高效性使它成为我们项目的理想选择。它能够有效处理事件总结、聚类总结和事件关系这三个核心任务，同时满足项目对精度和效率的双重需求。
  * 下面是 Baichuan 与其他竞品大模型在通用领域的 **5-shot 测试**

<img src="https://huia-image.oss-cn-guangzhou.aliyuncs.com/image-20240713174754906.png" alt="image-20240713174754906" style="zoom:70%;" />

#### 开发：Prompt 编写

完成模型选择后，接下来就是针对每个任务**编写 Prompt**。由于我们选择的模型参数量有限，因此 Prompt 的选择和调教就显得非常重要。在结合 Prompt 编写资料和实际实验结果，我总结了以下几点原则：

* **虚拟身份**
  * 给模型设定一个具体的角色或身份，比如新闻编辑、历史学者等。这样做可以帮助模型在生成文本时保持一致的语气和风格，使输出更加自然和专业。
  * 例如，如果模型扮演新闻编辑的角色，它在总结事件时会自然地采用更加客观和精炼的语言。
* **分点论述**
  * 将复杂的事件信息结构化为几个关键点。这不仅有助于模型更清晰地理解和处理信息，还能使最终的总结更加条理清晰，便于读者理解。
  * 例如：“请按照以下格式简洁明了地总结事件：1. 时间：… 2. 地点：… 3. 主要参与者：… 4. 关键点：… 5. 事件总结：… 6. 影响及后果：… 7. 评论观点：”。
* **举例说明**
  * 在Prompt中包括具体示例可以指导模型生成更具体、更贴近实际需求的内容。这可以作为模型生成文本的模板或框架，尤其是在处理复杂或多层次的信息时。
  * 示例可以是相关领域的典型事件总结，或是以往成功的总结案例，例如：“请按照以下格式简洁明了地总结事件：1. 时间：请提供事件发生的具体时间。2. 地点：请描述事件发生的具体地点。3. 主要参与者：请列出涉及的主要人物或团体。”

#### 后端通信：API

* **基于 Flask 的 Web 服务 API**

Flask 是一个轻量级的 Python web 框架，适合快速构建简单而强大的 web 应用。因为大模型需要更加强大且稳定的算力，因此我们选择将大模型放到GPU算力服务器上。因此，大模型所在服务器与后端不在同一台主机上。为此，我们基于 Flask 构建了一个轻量级 web 服务 API，用于后端与大模型进行通信。

考虑到大模型通常需要较强的计算能力和稳定性，我们选择将其部署在配备 GPU 的服务器上以提高处理效率。由于大模型运行在单独的服务器上，这就要求后端必须能够有效地与之通信。为了解决这一需求，我们基于 Flask 构建了一个轻量级 web 服务 API，用于后端与大模型进行通信。Flask 是一个轻量级的 Python web 框架，非常适合快速开发简单而强大的 web 应用，因此非常符合该项目服务 API 的需求。

<img src="https://huia-image.oss-cn-guangzhou.aliyuncs.com/image-20240713201837973.png" alt="image-20240713201837973" style="zoom:70%;" />

* **基于 Ngrok 的内网穿透**

由于我们的服务器位于受限的学院内网中，需要通过**端口转发**才能访问。为了克服这一挑战，我们选择使用 Ngrok 工具来实现内网穿透，从而使外部系统可以安全地访问学院服务器上的服务。Ngrok 是一种强大的服务，它通过建立一个安全的隧道到 Ngrok 的公共服务器，将本地端口转发到互联网上，使得内网中的服务可以被外界访问。

我们开发了一个专用的**内网穿透脚本**，使用 Ngrok 自动设置和维护这些隧道。这个脚本负责启动 Ngrok 服务，并将其指向我们的 Flask 应用端口。通过这种方式，尽管 Flask 应用部署在内网中，外部用户和系统依然可以通过从 Ngrok 获得的公网 URL 进行访问。

<img src="https://huia-image.oss-cn-guangzhou.aliyuncs.com/image-20240713201827577.png" alt="image-20240713201827577" style="zoom:70%;" />

### yzr

#### 调研阶段

针对事件抽取任务，对不同的事件抽取模型（如DeepKE、ERNIE-UIE等）进行调研，并在本地服务器部署。对不同的模型评估其效率、效果等。

#### 开发阶段

从零搭建项目前端部分，完成前端开发。

主要前端组件：

- Echarts：用于图表绘制
- DataV：用于非图表数据展示
- relation-graph：用于绘制事件关系图

为提高代码复用性、规范性、可读性等，将图表等可复用元素封装成组件。组件之间相互独立，提高前端程序的稳定性。